---
title: "IDX - Customer Retention Model"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: cosmo
  pdf_document:
    toc: yes
date: '2022-07-01'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
rm(list = ls())
```

# Introduction
In this project, we are working with a dataset called Telco Churn. The data includes many variables surrounding the demographic of customers of the Telco company. This project will take you step-by-step in the creation of a supervised learning model to predict whether a customer will churn or not. Our target variable is "Churn", which is a categorical variable that indicates whether a customer has churned or not.

```{r include = FALSE}
library(ggplot2)
library(dplyr)
library(inspectdf)
library(partykit)
library(rsample)
library(caret)
library(e1071)
```

# Data Understanding

In this step of exploration, I explored the variables of the Telco Churn data set. I tested for various variables and their relationship to our target variable "Churn".

Firstly, we must understand the data set we are working with.

- 7043 rows or observations that represent a customer of the company Telco.
- It also has 21 variables regarding the customer's behavior and demographic, both numeric and categorical.
* Demographic: age, marriage status, etc.
* Services: phone lines, internet services the customer is signed up for
* Customer Account: contract, payment method, etc.
* The "Churn" variable represents whether the customer has left in the past month or not.

```{r}
telco <- read.csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
```

Below is what the Telco dataset looks like including all of its variables.
```{r}
head(telco)
```

## Numerical Variables

### Monthly Charges

The first variable I will explore is monthly charges and it's relationship to Churn

```{r}
model1 <- lm(telco$MonthlyCharges ~ telco$Churn)

ggplot(data = telco,
       mapping = aes(x = Churn,
                     y = MonthlyCharges)) +
  geom_boxplot(fill = c("blue", "red"),
               alpha = 0.8) +
  xlab("Did not Churn vs. Churn") +
  ylab("Monthly Charges")
```
In the above chunk, I chose to create a boxplot between monthly charges and churn and visualized it in a scatter plot with a regression line. I found that customers who churned had higher average in monthly charges as compared to those who did not churn.

## Total Charges

The second variable I will explore is total charges and it's relationship to Churn

```{r}
model2 <- lm(telco$TotalCharges ~ telco$Churn)

ggplot(data = telco,
       mapping = aes(x = Churn,
                     y = TotalCharges)) +
  geom_boxplot(fill = c("blue", "red"),
               alpha = 0.8) +
  xlab("Did not Churn vs. Churn") +
  ylab("Total Charges")
```

In the above chunk, I chose to create a boxplot between total charges and churn and visualized it in a scatter plot with a regression line. I found that customers who churned had lower avaerage in total charges as compared to those who did not churn.

## Tenure

The third variable I will explore is tenure and it's relationship to Churn
```{r}
model3 <- lm(telco$tenure ~ telco$Churn)

ggplot(data = telco,
       mapping = aes(x = Churn,
                     y = tenure)) +
  geom_boxplot(fill = c("blue", "red"),
               alpha = 0.8) +
  xlab("Did not Churn vs. Churn") +
  ylab("Tenure")
```
In the above chunk, I chose to create a boxplot between tenure and churn and visualized it in a scatter plot with a regression line. I found that customers who churned had lower avaerage tenure as compared to those who did not churn.


## Categorical Variables

### Internet Service
```{r}
table(telco$Churn, telco$InternetService)
prop.table(table(telco$Churn[telco$Churn == "Yes"], telco$InternetService[telco$Churn == "Yes"]))
prop.table(table(telco$Churn[telco$Churn == "No"], telco$InternetService[telco$Churn == "No"]))
prop.table(table(telco$Churn[telco$InternetService == "DSL"], telco$InternetService[telco$InternetService == "DSL"]))
prop.table(table(telco$Churn[telco$InternetService == "Fiber optic"], telco$InternetService[telco$InternetService == "Fiber optic"]))
prop.table(table(telco$Churn[telco$InternetService == "No"], telco$InternetService[telco$InternetService == "No"]))
```
Here, I created proportion tables to see the the relationship of Churn and Internet Service. I found that customers with Fiber Optic Internet Service had the highest Churn Rate.

### Online Security
```{r}
table(telco$Churn, telco$OnlineSecurity)
prop.table(table(telco$Churn[telco$Churn == "Yes"], telco$OnlineSecurity[telco$Churn == "Yes"]))
prop.table(table(telco$Churn[telco$Churn == "No"], telco$OnlineSecurity[telco$Churn == "No"]))
prop.table(table(telco$Churn[telco$OnlineSecurity == "No"], telco$OnlineSecurity[telco$OnlineSecurity == "No"]))
prop.table(table(telco$Churn[telco$OnlineSecurity == "Yes"], telco$OnlineSecurity[telco$OnlineSecurity == "Yes"]))
```
Here, I created proportion tables to see the the relationship of Churn and Online Security. I found that customers with No Online Securtity had a higher Churn Rate as compared to those who have Online Security.

### Contract
```{r}
table(telco$Churn, telco$Contract)
prop.table(table(telco$Churn[telco$Churn == "Yes"], telco$Contract[telco$Churn == "Yes"]))
prop.table(table(telco$Churn[telco$Churn == "No"], telco$Contract[telco$Churn == "No"]))
prop.table(table(telco$Churn[telco$Contract == "Month-to-month"], telco$Contract[telco$Contract == "Month-to-month"]))
prop.table(table(telco$Churn[telco$Contract == "One year"], telco$Contract[telco$Contract == "One year"]))
```
Here, I created proportion tables to see the the relationship of Churn and Contract. I found that customers with Month-to-Month Contracts had a higher Churn Rate than those with One Year Contracts.

-----------------------------------------------------

# Data Preparation

## Missing Values
```{r}
inspect_na(telco)
```
Here I found that we have 11 missing values in the Total Charges Variable. However, this value is low as it makes up less than 5% of the data, so we can just remove it as below.

```{r}
telco <- na.omit(telco)
```

```{r}
inspect_na(telco)
```
Using the `inspect_na()` function, we can see that we no longer have missing values in our data.

-----------------------------------------------------
# Modelling

In creating our model, we need to use cross-validation in order to test the performance of our model. Thus, we need to divide our dataset into a training data set and a testing data set. To perform this portion of the project, we will use the rsample library.

```{r}
set.seed(888)

telco <- telco[ , -1]
telco$Churn <- ifelse(telco$Churn == "No", 0, 1)
```

```{r}
index <- initial_split(telco, prop = 0.7, strata = "Churn")

data_train <- training(index)
data_test <- testing(index)
```

Here, we set the seed to 888 to have a reproducible result, I set the seed to 888 and split the data into two sets using initial_split with prop = 0.7. This indicates that 70% of the observations in the Telco data will be used to create the model and 30% of the the observations will be used to test the performance of our model.

## Logistic Regression

The first method we will use is Logistic Regression or the Logit Model. In this model, the odds are modeled as a linear combination of our predictor variables

To create this logistic model, we use the `glm()` function for Churn against all other variables in our training data set.

```{r}
logistic_model <- glm(formula = Churn ~ ., data = data_train, family = "binomial")
```
To create this logistic model, we use the `glm()` function for Churn against all other variables in our training data set.

### Creating our Model Using Stepwise Variable Selection

To create a effective and efficient model, we need to choose variables that are statistically significant to predict the outcome of our target variable. Thus, we will use **Stepwise Regression** to cut out variables that are not statistically significant. Using `step()` function, we removed statistically insignificant variables to minimize the AIC score.

```{r}
model_stepwise <- step(logistic_model)
```

```{r}
summary(model_stepwise)
```
Above is the summary of our stepwise model.

### Model prediction & Evaluation

To test the performance of our model, we predict the values of our testing data set and compare it to the actual values to test its accuracy, precision and recall.

default output: log of odds ratio
type = "response" output: probability
```{r}
prob_output <- predict(object = model_stepwise, newdata = data_test, type = "response")
```

To predict the values of our logistic regression model, we use the `predict()` function with type = response to get probability predictions with values between 1 and 0.

```{r}
data_test$prob_output <- prob_output
data_test$prediction <- ifelse(data_test$prob_output > 0.5, 1, 0)
```

Because Churn has either value 0 or 1, we round our probability values to either 1 or 0. First, we tested a threshold of 0.5 as a cutoff between 1 and 0. Thus, if the probability generated by the `predict()` function is above 0.5, the customer will be predicted to churn and vice versa.

```{r}
table(actual = data_test$Churn,
      prediction = data_test$prediction)
```

The above table shows the predicted vs. actual churn values for the customers in the testing data set. Here we can see that our model correctly predicted that 1384 will churn and that 313 customers will not churn. However, it incorrectly predicted that 166 would churn and that 248 customers will not churn.

Because our data is imbalanced, we cannot rely on the accuracy alone to test the performance of our data. Thus, we need to check the Recall and Precision values. Recall is the sensitivity of our data and is measured by dividing the number of true positive cases by the number of actual positive cases. Precision is measured by dividing the number of true positive cases by the number of predicted positive cases.

Formulas:
Accuracy = correct/total
Recall (sensitivity) = true positive/actual positive
Precision (pos pred value) = true positive/predicted positive

```{r}
confusionMatrix(data = as.factor(data_test$prediction),
                reference = as.factor(data_test$Churn),
                positive = "1")
```

These were the values of the three measures using the `confusionMatrix()` function
- Accuracy = 0.8081
- Recall = 0.5651
- Precision = 0.6632


Here I decided to test a lower threshold of 0.4 as the cutoff value to see how it would affect the data.

```{r}
data_test$prediction <- ifelse(data_test$prob_output > 0.4, 1, 0)
```

```{r}
table(actual = data_test$Churn,
      prediction = data_test$prediction)
```

```{r}
confusionMatrix(data = as.factor(data_test$prediction),
                reference = as.factor(data_test$Churn),
                positive = "1")
```

These were the values of the three measures using the `confusionMatrix()` function

- Accuracy = 0.7938
- Recall = 0.6774
- Precision = 0.5994

Here we can see that the overall accuracy of the model and precision decreased but recall increased. This indicates that the model with a lower threshold is better at detecting positive cases where customers churn but also increases the cases of incorrectly predicted positive cases.

## Decision Tree
 
The second method we will use to create our model is the Decision Tree using the partykit package. The decision tree breaks down the data into smaller subsets by incrementally developing a decision tree based on the various variables. The decision tree has two types of nodes:
- decision nodes which branch out into subsets
- leaf nodes which classifies a final prediction or decision

### Creating our Model

Firstly, we must change our data into factors.
```{r}
data_train$Churn <- as.factor(data_train$Churn)
data_test$Churn <- as.factor(data_test$Churn)

data_train <- data_train %>%
  mutate_if(is.character, as.factor)

data_test <- data_test %>%
  mutate_if(is.character, as.factor)
```

&nbsp;

To create our decision tree model, we use the `ctree()` function for Churn against all other predictors.
```{r fig.width=10} 
model_dtree <- ctree(formula = Churn ~ .,
                     data = data_train)

plot(model_dtree)
```
Above, we can see how our decision tree model predicts the output of our churn.

### Model Evaluation & Prediction

```{r}
prediction_dtree <- predict(object = model_dtree,
                            newdata = data_test)
```

Once, again we will use the confusion matrix to check the values for accuracy, recall, and precision.
```{r}
confusionMatrix(data = prediction_dtree,
                reference = data_test$Churn,
                positive = "1")
```

- Accuracy = 0.7867
- Recall = 0.5134
- Precision = 0.6194

## Random Forest

The Random Forest is known as the most robust model, often producing the best quality predictions. The model uses k-folds and repeats to create a "forest" of decision trees. The model uses a voting mechanism and weighted combination on the prediction of its decision trees to make its final prediction.

### Creating our Model

Here we create an index using the `trainControl()` function to make our decision tree model and save it into a file for faster knitting. I first started with number = 8 and repeats = 5 as a baseline. The larger these numbers become, the longer the run time of the model creation.

```{r eval = FALSE}
set.seed(888)
ctrl <- trainControl(method = "repeatedcv",
                     number = 8,
                     repeats= 5)
model_rf <- train(Churn ~ .,
                  data = data_train,
                  method = "rf",
                  trControl = ctrl)
saveRDS(model_rf, "model_rf.RDS")
```

Here, we read the model we have written into an RDS file.
```{r}
model_rf1 <- readRDS("model_rf.RDS")
```

### Model Evaluation & Prediction

As we did for previous models, we create predictions on the testing data set to assess the performance and quality of our model.
```{r}
prediction_rf1 <- predict(object = model_rf1,
                         newdata = data_test)
```

To test the performance, we use a confusion matrix again.
```{r}
confusionMatrix(data = prediction_rf1,
                reference = data_test$Churn,
                positive = "1")
```

- Accuracy = 0.7848
- Recall = 0.5009
- Precision = 0.6167

The random forest model is a black box model, meaning that we cannot fully see its contents and decision making process. However, we can use the `varImp()` function to see which variables have the largest importance.
```{r}
varImp(object = model_rf1)
```

### Creating our Model pt.2

As I was unsatisfied with the Accuracy, Recall, and Precision values of the first Random Forest Model, I chose to create a second one with a higher value of number and repeats, to see whether it would improve the performance of the model significantly. This time, I used 10 and 8 respectively.

```{r eval = FALSE}
set.seed(888)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats= 8)
model_rf2 <- train(Churn ~ .,
                  data = data_train,
                  method = "rf",
                  trControl = ctrl)
saveRDS(model_rf2, "model_rf2.RDS")
```

```{r}
model_rf2 <- readRDS("model_rf2.RDS")
```

### Model Evaluation & Prediction Pt.2

Now, we assess the performance of this Random Forest Model with more K-folds and Repeats

```{r}
prediction_rf2 <- predict(object = model_rf2,
                         newdata = data_test)
```

```{r}
confusionMatrix(data = prediction_rf2,
                reference = data_test$Churn,
                positive = "1")
```

- Accuracy = 0.7825
- Recall = 0.4991
- Precision = 0.6114

These numbers show that increasing the numbers and repeats don't necessarily improve the performance of the model. Rather, we see that these values have decreased.

Using the `varImp()` function, the most important variables remain the same with slight variations in their overall value.
```{r}
varImp(object = model_rf2)
```

# Model Selection

In regards to selecting the best model, we can compare the values of Accuracy, Recall, and Precision of the models we have created. It can be seen in the table below.

| Model | Accuracy | Recall | Precision |
|:-----------------------|---------:|---------:|---------:|
| Logistic Regression 0.5 | 0.8081 | 0.5651 | 0.6632 |
| Logistic Regression 0.4 | 0.7938 | 0.6774 | 0.5994 |
| Decision Tree | 0.7867 | 0.5134 | 0.6194 |
| Random Forest 8/5 | 0.7848 | 0.5009 | 0.6167 |
| Random Forest 10/8 | 0.7825 | 0.4991 | 0.6114 |

As we can see from the table, the best performing model cannot be decided very easily but rather must be chosen based on the priority of the business we apply this model to.

The main conclusions I have reached:

- The two Random Forest models are the worst performing. Despite increasing its density and robustness, it's accuracy, precision, and recall values remain the lowest of the five models.
- The decision tree also falls behind in comparison to the two logistic regression models
- The logistic regression models are more promising performance based on their confusion matrix values and are the two best performing.

Thus, we must assess the company's priorities and business situation in order to choose between the two logistic regression models.

## Case 1: High Budget

If the company has a high budget and is in strong need to reduce churn, we would choose the 2nd Logistic Regression Model with a 0.4 Threshold.

| Model | Accuracy | Recall | Precision |
|:-----------------------|---------:|---------:|---------:|
| Logistic Regression 0.5 | 0.8081 | 0.5651 | 0.6632 |
| Logistic Regression 0.4 | 0.7938 | 0.6774 | 0.5994 |

In the above table, we can see that the Logistic Regression 0.4 model has a slightly lower accuracy and precision value, but has a far higher sensitivity score. This higher sensitivity score indicates that the model can predict positive values better, and thus warn the company of customers who are about to churn. In the case of *undisclosed company name*, this means that they will give promotions for all the customers who are predicted to churn. However, the model does have the lowest precision value, indicating that it has the highest rate of predicting false positives. Thus requiring a higher budget as there will be less profit margin for products purchased by these customers.

Concluding this section, if the company's priority is to keep as many customers as possible, Logistic Regression 0.4 Model is the best.

## Case 2: Lower Budget

If the company does not have as a high budget and needs a baseline model that can accurately predict customer churn, we would choose the 1st Logistic Regression model with a 0.5 Threshold.

| Model | Accuracy | Recall | Precision |
|:-----------------------|---------:|---------:|---------:|
| Logistic Regression 0.5 | 0.8081 | 0.5651 | 0.6632 |
| Logistic Regression 0.4 | 0.7938 | 0.6774 | 0.5994 |

As seen in the above table, the model has the highest accuracy and precision amongst all 5 models generated. It does not have the highest recall value, but it is the second highest, indicating that the model is still strong. In seeing so, if the company does not have a high budget, this model would be the best choice as the combination of its Accuracy, Recall, and Precision values are the best overall.